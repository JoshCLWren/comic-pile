# Worker Agent Retrospective - [Session/Task Name]

## 1. Outcome Summary

[Brief summary of what tasks were completed or attempted]

**Completed Tasks:** [List task IDs completed]
**Abandoned/Failed Tasks:** [List task IDs that failed or were abandoned]

**Cite completed task IDs:** [2-3 examples of completed tasks with brief context]

## 2. Task Execution & Understanding

**Did you understand the task requirements before starting implementation?** [Yes/No]

**Cite one task where requirements were clear and implementation was straightforward:** [Task ID]
[Explanation with specific example]

**Cite one task where requirements were ambiguous or difficult to interpret:** [Task ID or "None"]
[Explanation with specific example if applicable]

**Did you have to ask clarifying questions or seek additional information?** [Yes/No]
[If yes, cite the task and what information was needed]

## 3. Claiming & Task API Usage

**Did you claim the task before starting work?** [Yes/No]

**Did you maintain regular heartbeats while working?** [Yes/No]

**Did you update status notes at meaningful milestones?** [Yes/No]

**Cite one task with excellent API usage:** [Task ID]
[Example: "Claimed at 10:00, posted 'understanding requirements' note, posted 'implementing' note at 10:30, posted 'testing' note at 11:00, marked in_review at 11:30"]

**Cite one task with weak API usage:** [Task ID or "None"]
[Explanation if applicable]

## 4. Testing Quality

**Did you write tests for your implementation?** [Yes/No]

**Did all tests pass before marking in_review?** [Yes/No]

**Cite one task with excellent test coverage:** [Task ID]
[Example: "Added 5 new tests covering success path, error cases, edge cases. All 118 tests pass including 5 new ones"]

**Cite one task with insufficient testing:** [Task ID or "None"]
[Explanation if applicable]

**Did you run manual testing (browser verification, API endpoints, etc.)?** [Yes/No]
[Details of manual testing if performed]

## 5. Code Quality & Conventions

**Did code pass linting (ruff) before marking in_review?** [Yes/No]

**Did code pass type checking (pyright) before marking in_review?** [Yes/No]

**Did you follow existing code patterns and conventions?** [Yes/No]

**Cite one task where code quality was excellent:** [Task ID]
[Example: "Clean code following existing patterns, no linting errors, properly typed, no type: ignore comments"]

**Cite one task with code quality issues:** [Task ID or "None"]
[Explanation if applicable]

**Did you use `# type: ignore`, `# noqa`, or similar suppression comments?** [Yes/No]
[If yes, explain why - this should be rare and justified]

## 6. Communication & Progress Visibility

**Were your status notes sufficient for reviewer to understand progress without interrupting?** [Yes/No]

**Cite one task with excellent status notes:** [Task ID]
[Example showing clear progression from understanding → implementation → testing → completion]

**Cite one task with weak or missing status notes:** [Task ID or "None"]
[Explanation if applicable]

**Did you document files changed, test results, and manual testing performed?** [Yes/No]

## 7. Blocking Issues & Problem Solving

**List all blockers encountered:**

- [Task ID] - [Brief description of block]
  - duration: [X minutes/hours]
  - resolution: [How you resolved it]
  - cite: [Quote from your notes or explain]

**Did you mark tasks as blocked promptly when issues arose?** [Yes/No]

**Could any blocker have been prevented by better initial investigation?** [Yes/No with explanation]

**Cite one moment where you successfully unblocked yourself:** [Task ID and explanation]

## 8. Worktree Management

**Did you create the worktree before starting work?** [Yes/No]

**Did you work exclusively in the designated worktree?** [Yes/No]

**Did you clean up the worktree after task completion?** [Yes/No]

**Were there any worktree-related issues?** [Yes/No]
[Explanation if applicable]

**Cite one task where worktree management was handled well:** [Task ID]

## 9. Review Readiness & Handoff

**When you marked task in_review, was it actually ready for review?** [Yes/No]

**Did all of the following pass before marking in_review?**
- All tests pass: [Yes/No]
- Linting clean: [Yes/No]
- Type checking passes: [Yes/No]
- Migrations applied (if applicable): [Yes/No]
- Manual testing performed (if applicable): [Yes/No]

**Cite one task that was truly review-ready:** [Task ID]
[Example showing all checks passed and clear handoff notes]

**Did any task reach done while still missing:** [Testing/Linting/Documentation/etc.]
[Explanation if applicable]

**How would you rate your handoff quality to the reviewer?** [1-10 scale with justification]

## 10. Learning & Improvements (Actionable Only)

**What did you do well in this session that you want to continue?**
- [Specific pattern or practice that worked well]

**What would you do differently next time?**
- [Specific change to improve]

**List top 3 concrete changes to make before next task:**

1. [Specific improvement]
   - Would benefit: [What it improves]
   - Justification: [Why this matters]

2. [Specific improvement]
   - Would benefit: [What it improves]
   - Justification: [Why this matters]

3. [Specific improvement]
   - Would benefit: [What it improves]
   - Justification: [Why this matters]

**One new tool or workflow you would adopt:**
[Specific tool or workflow and why it would help]

**One thing you would stop doing:**
[Specific practice to avoid and why]

## 11. Final Verdict

**On a scale of 1–10, how confident are you that:**
- All completed tasks are correct: [X]/10
- Tests adequately cover your changes: [X]/10
- Code follows project conventions: [X]/10
- Communication was clear and timely: [X]/10

**Would you follow the same approach for your next task?** [Yes/No with explanation]

**One sentence of advice to a future worker agent, grounded in your experience:**

[Concrete advice based on what worked or didn't work in this session]
